{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "========\n",
    "@author: Matthew Rowley\n",
    "\n",
    "### Acknowledgements\n",
    "The whole idea for this notebook is taken from the excellent video series by YouTube Channel \"3Blue1Brown\". This series begins with the video: *But what **is** a Neural Network? | Deep learning, Part 1* (<https://www.youtube.com/watch?v=aircAruvnKk&t=1s>). The channel \"3Blue1Brown\" is a one-man labor of love produced by the talented Grant Sanderson. (actually, since I wrote this Grant has hired some help, but he is still the creative genius behind the channel)\n",
    "\n",
    "### Description\n",
    "This notebook will explore the implementation and training of a neural network. The network will take images of hand-drawn numerals as inputs, and output guesses for which numeral was drawn.\n",
    "\n",
    "Following Grant's lead, the network will have two hidden layers, each with 16 nodes. I also weighed the true value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import idx2numpy\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create an instance of the neural network with random weights and zero biases, and test it on a random input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"The Neural Network Class\"\"\"\n",
    "    def __init__(self, W1=None, W2=None, W3=None, B2=None, B3=None, B4=None, ReLU=False, variability=None):\n",
    "        \"\"\"Initialization routine generates random values\"\"\"\n",
    "        if(W1 is None): W1 = np.random.random([784,16]) - 0.5\n",
    "        if(W2 is None): W2 = np.random.random([16,16]) - 0.5\n",
    "        if(W3 is None): W3 = np.random.random([16,10]) - 0.5\n",
    "        if(B2 is None): B2 = np.random.random(16) - 0.5\n",
    "        if(B3 is None): B3 = np.random.random(16) - 0.5\n",
    "        if(B4 is None): B4 = np.random.random(10) - 0.5\n",
    "        if(variability is None): variability = 0.1\n",
    "        self.variability = variability  # How much to randomize weights and biases in the randomize function\n",
    "        self.W1=W1  # Weights\n",
    "        self.W2=W2\n",
    "        self.W3=W3\n",
    "        self.B2=B2  # Node Biases\n",
    "        self.B3=B3\n",
    "        self.B4=B4\n",
    "        self.N1 = np.zeros(784) # Nodes (initialized as zeros)\n",
    "        self.N2 = np.zeros(16)\n",
    "        self.N3 = np.zeros(16)\n",
    "        self.N4 = np.zeros(10)\n",
    "        self.ReLU = ReLU\n",
    "        self.error = 0  # Metric for the quality of this network on the training set, cumulative over all training samples\n",
    "        \n",
    "    def identifyNumber(self, N1=None, trueValue=None):\n",
    "        \"\"\"Given an array of pixel values, give the most probable numeral\"\"\"\n",
    "        if (N1 is None): N1 = np.random.random(784) # default to an image of random noise\n",
    "        if (trueValue is None): trueValue = int(np.random.random(1)*10) # default to a random true value\n",
    "        self.N1 = N1\n",
    "        self.N2 = self.normalize(np.dot(self.N1, self.W1) - self.B2)\n",
    "        self.N3 = self.normalize(np.dot(self.N2, self.W2) - self.B3)\n",
    "        self.N4 = self.normalize(np.dot(self.N3, self.W3) - self.B4)\n",
    "        max_val = 0\n",
    "        max_i = 0\n",
    "        for i, val in enumerate(self.N4):\n",
    "            if val > max_val:\n",
    "                max_val = val\n",
    "                max_i = i\n",
    "        for i in range(10):\n",
    "            if i == trueValue:  # For the true value, probability should be close to 1 \n",
    "                self.error += 2.5*(1.0-self.N4[i])**2  # Weigh the correct answer 2.5 times more than others\n",
    "            else:  # For all others, probability should be close to 0\n",
    "                self.error += (0.0-self.N4[i])**2\n",
    "        return max_i, self.N4, self.error # this is (numeral, [probability values for numerals 0-9], error)\n",
    "        \n",
    "    def normalize(self, nodeVals):\n",
    "        \"\"\"Normalize the node values according to a sigmoid or ReLU function\"\"\"\n",
    "        if(self.ReLU):\n",
    "            return nodeVals.clip(min=0)\n",
    "        else:\n",
    "            return 1.0 / (1.0 + np.exp(-nodeVals))\n",
    "    \n",
    "    def randomize(self):\n",
    "        \"\"\"Modify the current networks parameters by small random values\"\"\"\n",
    "        self.W1 = self.W1 + (np.random.random([784,16]) - 0.5)*self.variability\n",
    "        self.W2 = self.W2 + (np.random.random([16,16]) - 0.5)*self.variability\n",
    "        self.W3 = self.W3 + (np.random.random([16,10]) - 0.5)*self.variability\n",
    "        self.B2 = self.B2 + (np.random.random(16) - 0.5)*self.variability\n",
    "        self.B3 = self.B3 + (np.random.random(16) - 0.5)*self.variability\n",
    "        self.B4 = self.B4 + (np.random.random(10) - 0.5)*self.variability\n",
    "        \n",
    "    def clone(self):\n",
    "        \"\"\"A function to return a cloned network - with the same weights and offsets as this one\"\"\"\n",
    "        my_clone = NeuralNetwork(W1=np.copy(self.W1), W2=np.copy(self.W2), W3=np.copy(self.W3),\n",
    "                                 B2=np.copy(self.B2), B3=np.copy(self.B3), B4=np.copy(self.B4))\n",
    "        return my_clone\n",
    "    \n",
    "    def resetError(self):\n",
    "        \"\"\"Reset the cumulative error variable to 0\"\"\"\n",
    "        self.error=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNetwork=NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, array([0.67236826, 0.65166649, 0.42549631, 0.39339571, 0.27215409,\n",
      "       0.46897357, 0.63070417, 0.85280158, 0.78170571, 0.1287161 ]), 3.2024103641032298)\n"
     ]
    }
   ],
   "source": [
    "print(myNetwork.identifyNumber())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### I've drawn a \"6.\" Here I import it, and run it on the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(os.path.join(\"Data\",\"Test.png\"), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, array([0.66068723, 0.61452162, 0.4120595 , 0.3877832 , 0.2743762 ,\n",
      "       0.48003389, 0.62309054, 0.85734742, 0.7694498 , 0.1147115 ]), 6.337846658864208)\n"
     ]
    }
   ],
   "source": [
    "im = np.ndarray.flatten(im)/255.0\n",
    "print(myNetwork.identifyNumber(N1=im, trueValue=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I will set up a short script for optimizing a network using a single training sample and a roughly genetic-esque optimization routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = [0,0,0,0,0,0,0,0,0,0]  # create a \"generation\" of 10 individuals\n",
    "for i in range(len(networks)):\n",
    "    networks[i] = NeuralNetwork()  # Instantiate each individual as a random neural network\n",
    "for network in networks:  \n",
    "    network.identifyNumber(N1=im, trueValue=6)  # Test each network on the sample to give it an error value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having set up and run the network once, I am ready to \"Train\" it to recognize a 6. Run the cell below as many times as necessary to get very good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Error, index: 0.24118904178433456, 9\n"
     ]
    }
   ],
   "source": [
    "min_i = 0\n",
    "min_val = networks[0].error\n",
    "for i, network in enumerate(networks):  # Find the index for the individual in this generation with the lowest error\n",
    "    if network.error < min_val:\n",
    "        min_i = i\n",
    "        min_val = network.error\n",
    "print(\"Best Error, index: {}, {}\".format(min_val, min_i))\n",
    "W1 = networks[min_i].W1\n",
    "W2 = networks[min_i].W2\n",
    "W3 = networks[min_i].W3\n",
    "B2 = networks[min_i].B2\n",
    "B3 = networks[min_i].B3\n",
    "B4 = networks[min_i].B4\n",
    "for i in range(len(networks)):  # Create a new generation of clones of the best network so far\n",
    "    networks[i] = NeuralNetwork(W1=W1, W2=W2, W3=W3, B2=B2, B3=B3, B4=B4)\n",
    "for i, network in enumerate(networks):\n",
    "    if (i != min_i):  # Preserve the best individual from the last generation and randomize all others  \n",
    "        network.randomize()\n",
    "    network.identifyNumber(N1=im, trueValue=6)  # Test the new generation of networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, array([0.16395654, 0.12846182, 0.15337184, 0.14238429, 0.17579153,\n",
       "        0.18848052, 0.91319048, 0.11418974, 0.22249144, 0.11111472]), 0.49467182507164564)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networks[0].identifyNumber(N1=im, trueValue=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Of course, here I have grossly \"overfit\" and there is no likelihood that the network is doing any image processing at all, but rather gaming the numbers to always give a 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Images\n",
    "\n",
    "#### I need many images (thousands) to adequately train the network. Thankfully, I can rely on the databases provided by Drs. LeCun and Cortes at: <http://yann.lecun.com/exdb/mnist/>\n",
    "\n",
    "#### The databases include two sets of images (60,000 training and 10,000 testing), and two sets of true-value labels (training and testing). The training and testing sets are similar in every way, but just include different examples of handwritten numbers. By keeping them separate (i.e. *never* training with the testing set), we can be sure that our networks are able to recognize numbers outside of their training set.\n",
    "\n",
    "#### The image arrays must also be flattened so that the image data is a 1-D array (rather than a 2-d one) and normalized to a maximum value of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unflattened_training_images = idx2numpy.convert_from_file(os.path.join(\"Data\",'train-images.idx3-ubyte'))\n",
    "training_labels = idx2numpy.convert_from_file(os.path.join(\"Data\",'train-labels.idx1-ubyte'))\n",
    "unflattened_testing_images = idx2numpy.convert_from_file(os.path.join(\"Data\",'t10k-images.idx3-ubyte'))\n",
    "testing_labels = idx2numpy.convert_from_file(os.path.join(\"Data\",'t10k-labels.idx1-ubyte'))\n",
    "print(unflattened_training_images.shape)\n",
    "training_images = np.empty([60000, 784])\n",
    "testing_images = np.empty([10000, 784])\n",
    "for i in range(60000):\n",
    "    training_images[i] = np.ndarray.flatten(unflattened_training_images[i]) / 255.0\n",
    "for i in range(10000):\n",
    "    testing_images[i] = np.ndarray.flatten(unflattened_testing_images[i]) / 255.0\n",
    "print(training_images.shape)\n",
    "# Clean up some memory\n",
    "unflattened_training_images = None\n",
    "unflattened_testing_images = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideally, a trained network would give high probability to the true answer, and low probabilities to all other answers. I found that even reliable networks would give fairly high probabilities to *all* numbers, and the correct number would just barely beat out the others. I don't really know if this is a problem, but it bothered me so I came up with a method to discourage the network from making overconfident guesses. Here I append some images with random data and a \"99\" true-value label. These images will never be marked correct, so the network is rewarded for recoginzing when an image is not a number at all, and returning low confidence values for all numerals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66000, 784)\n",
      "(66000,)\n"
     ]
    }
   ],
   "source": [
    "training_images = np.concatenate((training_images, np.random.rand(6000,784)))\n",
    "training_labels = np.concatenate((training_labels, 99.0 * np.ones(6000)))\n",
    "\n",
    "testing_images = np.concatenate((testing_images, np.random.rand(1000,784)))\n",
    "testing_labels = np.concatenate((testing_labels, 99.0 * np.ones(1000)))\n",
    "\n",
    "print(training_images.shape)\n",
    "print(training_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network the slow way\n",
    "\n",
    "#### Now we can train the network using my simple genetic algorithm demonstrated above, and a random sample of 500 images from the training data\n",
    "\n",
    "#### It will converge very slowly due to the enormous number of parameters and the randomness of the walk toward a local minimum, but it was relatively easy to code\n",
    "\n",
    "#### Another advantage to this algorithm is that it parallellizes easily, but I haven't bothered with that here\n",
    "\n",
    "#### The training set is too large to train each generation on the whole set, but we don't want to overtrain on a single small sample of the set. So, each generation selects a new random sample of training images from the whole collection\n",
    "\n",
    "#### First, I set up the generational structure, this time with 50 individuals per generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5215063002334722\n",
      "2.49454599492054\n",
      "3.333975887321924\n",
      "3.085988107849417\n",
      "3.1184290445502483\n",
      "3.6027516039475214\n",
      "3.8282239726667564\n",
      "3.0161096906201705\n",
      "2.9814189178171717\n",
      "3.3954051459525245\n",
      "3.0691021853498723\n",
      "3.1573170169640092\n",
      "3.1857467721091903\n",
      "3.641016229138962\n",
      "3.0697561214336786\n",
      "3.244727720374784\n",
      "3.473540500771438\n",
      "3.0532785706714547\n",
      "2.9330318052575652\n",
      "2.9092709025123\n",
      "3.2270412875834364\n",
      "2.7349647332858646\n",
      "2.960290007645651\n",
      "2.839173800407695\n",
      "3.217568646569299\n",
      "4.609163923660004\n",
      "2.8107678945260863\n",
      "2.7731143992085254\n",
      "3.8415748613317446\n",
      "3.483121414037572\n",
      "2.4550846694977815\n",
      "2.7031090937536577\n",
      "2.5423021971507014\n",
      "3.345832819550705\n",
      "2.801765565423807\n",
      "3.099957769605241\n",
      "3.2317306764650766\n",
      "3.6585661391018056\n",
      "3.11900147345297\n",
      "2.78553652762927\n",
      "2.7742257129931858\n",
      "2.86758721649433\n",
      "2.733456750574516\n",
      "3.2954562047160243\n",
      "2.9104051802153914\n",
      "3.07439532852658\n",
      "3.1846171628772617\n",
      "3.2666017976172252\n",
      "3.4145008992305206\n",
      "3.1647903738974077\n",
      "Best Error, index: 0.5215063002334722, 0\n"
     ]
    }
   ],
   "source": [
    "sample_number = 20000  # How many images to train on\n",
    "load_best = True  # If a champion network is saved from a previous session, it can be loaded and included here\n",
    "networks = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(len(networks)):\n",
    "    networks[i] = NeuralNetwork()\n",
    "if load_best:  # Load the saved network into the 0 index for this generation\n",
    "    loadfile = os.path.join(\"Data\", \"Best.pkl\")\n",
    "    with open(loadfile, 'rb') as pickled_network:\n",
    "        networks[0] = pickle.load(pickled_network, encoding='latin1')\n",
    "        networks[0].resetError()  # Reset the error, in case it is held over from previous trainings\n",
    "trainers = np.random.randint(0,65999,sample_number)  # Here we randomly select some images from the training set\n",
    "for network in networks:\n",
    "    for index in trainers:\n",
    "        network.identifyNumber(N1=training_images[index], trueValue=training_labels[index])\n",
    "min_i = 0\n",
    "min_val = networks[0].error\n",
    "for i, network in enumerate(networks):\n",
    "    if network.error < min_val:\n",
    "        min_i = i\n",
    "        min_val = network.error\n",
    "min_val = min_val / sample_number\n",
    "for network in networks:\n",
    "    print(network.error / sample_number)\n",
    "print(\"Best Error, index: {}, {}\".format(min_val, min_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, there will be some variation in the error values due to the different samples in the training set. Ideally, we would use a large enough sample_number so that the differences between training sample sets are small, but use a samll enough sample_number so that the training proceeds quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors Standard Deviation: 0.006762171728391975\n"
     ]
    }
   ],
   "source": [
    "sample_number = 20000\n",
    "errors = np.zeros(100)\n",
    "for i in range(100):\n",
    "    trainers = np.random.randint(0,65999,sample_number)  # Here we randomly select some images from the training set\n",
    "    networks[min_i].resetError()\n",
    "    for index in trainers:\n",
    "        networks[min_i].identifyNumber(N1=training_images[index], trueValue=training_labels[index])\n",
    "    errors[i] = networks[min_i].error / sample_number\n",
    "print(\"Errors Standard Deviation: {}\".format(np.std(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also need to choose an appropriate range to vary values over. If we keep the same set of training images but randomize some of the variables, we can see how much the error varies under randomization. This value should be greater than the standard deviation above, but ideally not too large. Maybe about twice as large. This will ensure that a new network will likely only outperform the current champion based on actual improvement in the weights and biases, not bacause of the variation between training sample subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors Standard Deviation: 0.009228342802777645\n"
     ]
    }
   ],
   "source": [
    "target_error = 0.5\n",
    "min_val = 0.52\n",
    "trainers = np.random.randint(0,65999,sample_number)\n",
    "v_errors = np.zeros(100)\n",
    "for i in range(100):\n",
    "    network = networks[min_i].clone()\n",
    "    network.variability = (1 - target_error/min_val)**2*400\n",
    "    network.randomize()\n",
    "    for index in trainers:\n",
    "        network.identifyNumber(N1=training_images[index], trueValue=training_labels[index])\n",
    "    v_errors[i] = network.error / sample_number\n",
    "print(\"Errors Standard Deviation: {}\".format(np.std(v_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the network is initialized, run the cell below to optimize until a target error value is reached\n",
    "\n",
    "#### I have also set the variability to start large and diminish as the networks converge to the target error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Error, index: 0.5083064473964579, 0\n",
      "Best Error, index: 0.5145689416729682, 40\n",
      "Best Error, index: 0.5203577393245329, 0\n",
      "Best Error, index: 0.5161638467304877, 0\n",
      "Best Error, index: 0.5183822713342823, 0\n",
      "Best Error, index: 0.5157293719696647, 0\n",
      "Best Error, index: 0.5112078546793009, 0\n",
      "Best Error, index: 0.5119283616098627, 0\n",
      "Best Error, index: 0.5213163063343244, 0\n",
      "Best Error, index: 0.5266270184091586, 0\n",
      "Best Error, index: 0.520833121278936, 0\n",
      "Best Error, index: 0.5174301434165369, 0\n",
      "Best Error, index: 0.5244092991751279, 0\n",
      "Best Error, index: 0.5224062264613547, 0\n",
      "Best Error, index: 0.5056841157689633, 0\n",
      "Best Error, index: 0.5283783233130219, 28\n",
      "Best Error, index: 0.5177785502842542, 0\n",
      "Best Error, index: 0.5197023094410425, 0\n",
      "Best Error, index: 0.516102610991342, 0\n",
      "Best Error, index: 0.5148799982330906, 0\n",
      "Best Error, index: 0.5128952622793153, 0\n",
      "Best Error, index: 0.5189479113321548, 0\n",
      "Best Error, index: 0.5241842220385529, 0\n",
      "Best Error, index: 0.5265243748521553, 0\n",
      "Best Error, index: 0.5287458723957021, 0\n",
      "Best Error, index: 0.519775096018316, 0\n",
      "Best Error, index: 0.5219873679414859, 0\n",
      "Best Error, index: 0.5213944279546726, 0\n",
      "Best Error, index: 0.5290373920439384, 0\n",
      "Best Error, index: 0.525109720503029, 0\n",
      "Best Error, index: 0.5249461036782976, 0\n",
      "Best Error, index: 0.5218983373504615, 0\n",
      "Best Error, index: 0.5183048822814708, 0\n",
      "Best Error, index: 0.5150550352831501, 0\n",
      "Best Error, index: 0.5226542486851916, 0\n",
      "Best Error, index: 0.5191479678792356, 0\n",
      "Best Error, index: 0.5129519677426236, 0\n",
      "Best Error, index: 0.5396495347094846, 0\n",
      "Best Error, index: 0.5111572284079251, 0\n",
      "Best Error, index: 0.5269295378403287, 0\n",
      "Best Error, index: 0.5256806142647481, 0\n",
      "Best Error, index: 0.5202866978527796, 0\n",
      "Best Error, index: 0.5116372538259363, 0\n",
      "Best Error, index: 0.5262857701728343, 0\n",
      "Best Error, index: 0.5229073201337984, 0\n",
      "Best Error, index: 0.5156288273395311, 0\n",
      "Best Error, index: 0.5202718736147627, 0\n",
      "Best Error, index: 0.5139899219641467, 0\n",
      "Best Error, index: 0.5247816029318968, 0\n",
      "Best Error, index: 0.5157130022113618, 0\n",
      "Best Error, index: 0.5199554644186251, 0\n",
      "Best Error, index: 0.5166970374839492, 0\n",
      "Best Error, index: 0.521618542718369, 0\n",
      "Best Error, index: 0.5245496984803016, 0\n",
      "Best Error, index: 0.5121223276274739, 0\n",
      "Best Error, index: 0.5322340398865354, 0\n",
      "Best Error, index: 0.5133304026773305, 0\n",
      "Best Error, index: 0.5238030587040078, 0\n",
      "Best Error, index: 0.5316876339024617, 0\n",
      "Best Error, index: 0.5171942145279526, 0\n",
      "Best Error, index: 0.521706685265545, 0\n",
      "Best Error, index: 0.5137548552908151, 0\n",
      "Best Error, index: 0.5170627212472872, 0\n",
      "Best Error, index: 0.5253772500351425, 0\n",
      "Best Error, index: 0.5209509012737216, 0\n",
      "Best Error, index: 0.5274767050025831, 0\n",
      "Best Error, index: 0.5209636967254166, 0\n",
      "Best Error, index: 0.5214471893207188, 0\n",
      "Best Error, index: 0.5168332007676483, 0\n",
      "Best Error, index: 0.5236876020510168, 0\n",
      "Best Error, index: 0.5197868276601308, 0\n",
      "Best Error, index: 0.5118045413690555, 0\n",
      "Best Error, index: 0.5205465995558904, 0\n",
      "Best Error, index: 0.5145572475894207, 0\n",
      "Best Error, index: 0.525706797797858, 0\n",
      "Best Error, index: 0.5266192378192706, 0\n",
      "Best Error, index: 0.531291338459443, 0\n",
      "Best Error, index: 0.5188571740093209, 0\n",
      "Best Error, index: 0.5163343784429842, 0\n",
      "Best Error, index: 0.520509212497166, 0\n",
      "Best Error, index: 0.520756889368343, 0\n",
      "Best Error, index: 0.5280012282598087, 0\n",
      "Best Error, index: 0.5222912385215661, 0\n",
      "Best Error, index: 0.5152655472549252, 0\n",
      "Best Error, index: 0.5025610531616755, 0\n",
      "Best Error, index: 0.5195861266692355, 11\n",
      "Best Error, index: 0.5301138952388266, 0\n",
      "Best Error, index: 0.5192940871643156, 0\n",
      "Best Error, index: 0.5064568765430361, 0\n",
      "Best Error, index: 0.5232393677805423, 30\n",
      "Best Error, index: 0.5254977425037697, 0\n",
      "Best Error, index: 0.515259261328782, 0\n",
      "Best Error, index: 0.5320122940186488, 0\n",
      "Best Error, index: 0.510615763295017, 0\n",
      "Best Error, index: 0.5162700849324392, 0\n",
      "Best Error, index: 0.5139294018714208, 0\n",
      "Best Error, index: 0.5261122259535262, 0\n",
      "Best Error, index: 0.5159400733619651, 0\n",
      "Best Error, index: 0.5096505804639554, 0\n",
      "Best Error, index: 0.5269426155527621, 28\n",
      "Best Error, index: 0.5187510373380151, 0\n",
      "Best Error, index: 0.5247147975250289, 0\n",
      "Best Error, index: 0.5335156672917745, 0\n",
      "Best Error, index: 0.5117083068869531, 0\n",
      "Best Error, index: 0.5210856638496921, 34\n",
      "Best Error, index: 0.519046867181566, 0\n",
      "Best Error, index: 0.5195791150420378, 0\n",
      "Best Error, index: 0.523896162581801, 0\n",
      "Best Error, index: 0.5180994705771185, 0\n",
      "Best Error, index: 0.513552925330656, 0\n",
      "Best Error, index: 0.5234708391884574, 0\n",
      "Best Error, index: 0.5111769594830352, 0\n",
      "Best Error, index: 0.5111882508340011, 0\n",
      "Best Error, index: 0.5139713449785033, 0\n",
      "Best Error, index: 0.5188282643842508, 0\n",
      "Best Error, index: 0.5174747718934054, 0\n",
      "Best Error, index: 0.5272548867528959, 0\n",
      "Best Error, index: 0.5237178023800898, 0\n",
      "Best Error, index: 0.5194423898042544, 0\n",
      "Best Error, index: 0.5174535851557817, 0\n",
      "Best Error, index: 0.5169909443242261, 0\n",
      "Best Error, index: 0.5283250175871259, 0\n",
      "Best Error, index: 0.5284364119945344, 0\n",
      "Best Error, index: 0.5268916186910905, 0\n",
      "Best Error, index: 0.524264821364007, 0\n",
      "Best Error, index: 0.5116258224905099, 0\n",
      "Best Error, index: 0.5104382795234353, 0\n",
      "Best Error, index: 0.5037979948858079, 0\n",
      "Best Error, index: 0.511060745860103, 45\n",
      "Best Error, index: 0.517908656591211, 31\n",
      "Best Error, index: 0.5184922566658211, 0\n",
      "Best Error, index: 0.5267046390403728, 0\n",
      "Best Error, index: 0.5161685555895342, 0\n",
      "Best Error, index: 0.5166551500647361, 0\n",
      "Best Error, index: 0.5158027116291393, 0\n",
      "Best Error, index: 0.5125508259858105, 0\n",
      "Best Error, index: 0.5190383249225483, 0\n",
      "Best Error, index: 0.5243667989558225, 0\n",
      "Best Error, index: 0.5234280919698453, 0\n",
      "Best Error, index: 0.5218558758537345, 0\n",
      "Best Error, index: 0.5130206468990174, 0\n",
      "Best Error, index: 0.5222287444170252, 0\n",
      "Best Error, index: 0.5250703540866384, 0\n",
      "Best Error, index: 0.5181826187311745, 0\n",
      "Best Error, index: 0.5211853465871569, 0\n",
      "Best Error, index: 0.5072031446116527, 0\n",
      "Best Error, index: 0.522072127898827, 40\n",
      "Best Error, index: 0.5313619706655429, 0\n",
      "Best Error, index: 0.5130698878424962, 0\n",
      "Best Error, index: 0.5169029594974144, 0\n",
      "Best Error, index: 0.5276263338518284, 0\n",
      "Best Error, index: 0.5279842772727196, 0\n",
      "Best Error, index: 0.511172025293951, 0\n",
      "Best Error, index: 0.5181031934954553, 5\n",
      "Best Error, index: 0.5167353584830386, 0\n",
      "Best Error, index: 0.5144018623285541, 0\n",
      "Best Error, index: 0.5168717464686413, 0\n",
      "Best Error, index: 0.5144815738288869, 0\n",
      "Best Error, index: 0.5241452109849338, 0\n",
      "Best Error, index: 0.5184723110204733, 0\n",
      "Best Error, index: 0.5206951701514411, 0\n",
      "Best Error, index: 0.5272952625438049, 0\n",
      "Best Error, index: 0.5227967599605781, 0\n",
      "Best Error, index: 0.5126458234446257, 0\n",
      "Best Error, index: 0.5216577266229806, 0\n",
      "Best Error, index: 0.5195603764810802, 0\n",
      "Best Error, index: 0.5212791755485442, 0\n",
      "Best Error, index: 0.5198105041312088, 0\n",
      "Best Error, index: 0.51960606022265, 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-b7d4e860ef5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifyNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrueValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmin_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmin_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f5734581f654>\u001b[0m in \u001b[0;36midentifyNumber\u001b[0;34m(self, N1, trueValue)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrueValue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrueValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# default to a random true value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_error = .5  # The algorithm will stop once the error is equal to or less than this threshold\n",
    "while(min_val > target_error):\n",
    "    W1 = networks[min_i].W1\n",
    "    W2 = networks[min_i].W2\n",
    "    W3 = networks[min_i].W3\n",
    "    B2 = networks[min_i].B2\n",
    "    B3 = networks[min_i].B3\n",
    "    B4 = networks[min_i].B4\n",
    "    for i in range(len(networks)):\n",
    "        networks[i] = NeuralNetwork(W1=W1, W2=W2, W3=W3, B2=B2, B3=B3, B4=B4,\n",
    "                                    variability = (1 - target_error/min_val)**2*400)\n",
    "    trainers = np.random.randint(0,65999,sample_number) # Get new training data with each iteration\n",
    "    for i, network in enumerate(networks):\n",
    "        if (i != 0):\n",
    "            network.randomize()\n",
    "        for index in trainers:\n",
    "            network.identifyNumber(N1=training_images[index], trueValue=training_labels[index])\n",
    "    min_i = 0\n",
    "    min_val = networks[0].error\n",
    "    for i, network in enumerate(networks):\n",
    "        if network.error < min_val:\n",
    "            min_i = i\n",
    "            min_val = network.error\n",
    "    min_val = min_val / sample_number\n",
    "    print(\"Best Error, index: {}, {}\".format(min_val, min_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After a full day of number-crunching, the algorithm failed to reach a target error of 0.5, so I stopped it. The best performer is still a fairly well-trained network, so let's test its accuracy on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Mistakes: 1987 out of 11000\n",
      "Error: 0.5011689078124036\n"
     ]
    }
   ],
   "source": [
    "mistakes = 0\n",
    "set_size = testing_images.shape[0]\n",
    "networks[min_i].resetError()\n",
    "for image, label in zip(testing_images, testing_labels):\n",
    "    guess, probs, error = networks[min_i].identifyNumber(N1=image, trueValue=label)\n",
    "    if(label == 99):  # These are the random images, the network should have low confidence all around\n",
    "        if(np.max(probs)>0.5): mistakes = mistakes + 1  # high confidence in any numeral counts as an error\n",
    "    else:  # These are real images, the guess should match the label\n",
    "        if(guess != label): mistakes = mistakes + 1\n",
    "print(\"Total Mistakes: {} out of {}\".format(mistakes, set_size))\n",
    "print(\"Error: {}\".format(networks[min_i].error / set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, 2012 out of 11000 isn't bad, but a more directed optimization algorithm might be able to improve the network better than the random walk used above. Proper gradient descent involves finding the steepest slope within the full multi-dimensional parameter space, and proceeding down that slope. A cheap and easy to code substitute will look at each parameter one at a time, rather than the whole multi-dimensional space. Find the partial derivative of the error with respect to one parameter, and change that parameter if necessary to reduce the error, then move on to the next parameter and so on. To prevent any randomness in this walk toward the minimum, we will use the entire training set.\n",
    "\n",
    "#### Although this algorithm is guaranteed to proceed only to lower error, never taking a step backward, it will take an incredibly long time to complete even one cycle. This is because of the enormous number of parameters. W1, for example, includes 784*16=12544 individual weights. \n",
    "\n",
    "#### First, we save the work from above. Then, initialize a best_network variable to use in the new algorithm (we won't be using 50 individuals per generation any more from here onward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks[min_i].resetError()\n",
    "savefile = os.path.join(\"Data\", \"Best.pkl\")\n",
    "with open(savefile, 'wb') as output:\n",
    "    pickle.dump(networks[min_i], output)\n",
    "best_network = networks[min_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell below is designed to be run over and over again until a cycle completes without changing any of the parameters (because they are at a local minimum). Then, either make the step size smaller or take the network as good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-2754c3666af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mbest_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifyNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrueValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mincrease\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifyNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrueValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mdecrease\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifyNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrueValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrease\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mincrease\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdecrease\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mbest_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincrease\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f5734581f654>\u001b[0m in \u001b[0;36midentifyNumber\u001b[0;34m(self, N1, trueValue)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmax_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mmax_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step_size = 0.01  # This is a relative change in the parameter magnitude\n",
    "changes = 0\n",
    "verbose = True  # Set this True to print every improvement. Set it False to run silently until the end\n",
    "# W1 is 784,16 -- This is simply too big to go over every weight. But we can't simply ignor it either\n",
    "#                 For now, randomly select 200 weights and hope they make some difference\n",
    "i_vals = np.random.randint(low=0, high=783, size=200)\n",
    "j_vals = np.random.randint(low=0, high=15, size=200)\n",
    "for num in range(200):\n",
    "    i = i_vals[num]\n",
    "    j = j_vals[num]\n",
    "    weight = best_network.W1[i][j]\n",
    "    best_network.resetError()\n",
    "    increase = best_network.clone()\n",
    "    increase.W1[i][j] = (1+step_size)*weight\n",
    "    decrease = best_network.clone()\n",
    "    decrease.W1[i][j] = (1-step_size)*weight\n",
    "    for image, label in zip(training_images, training_labels):\n",
    "        best_network.identifyNumber(N1=image, trueValue=label)\n",
    "        increase.identifyNumber(N1=image, trueValue=label)\n",
    "        decrease.identifyNumber(N1=image, trueValue=label)\n",
    "    if(increase.error < best_network.error and increase.error < decrease.error):\n",
    "        best_network = increase\n",
    "        changes = changes + 1\n",
    "        if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "    elif(decrease.error < best_network.error and decrease.error < increase.error):\n",
    "        best_network = decrease\n",
    "        changes = changes + 1\n",
    "        if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "# W2 is 16x16\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        weight = best_network.W2[i][j]\n",
    "        best_network.resetError()\n",
    "        increase = best_network.clone()\n",
    "        increase.W2[i][j] = (1+step_size)*weight\n",
    "        decrease = best_network.clone()\n",
    "        decrease.W2[i][j] = (1-step_size)*weight\n",
    "        for image, label in zip(training_images, training_labels):\n",
    "            best_network.identifyNumber(N1=image, trueValue=label)\n",
    "            increase.identifyNumber(N1=image, trueValue=label)\n",
    "            decrease.identifyNumber(N1=image, trueValue=label)\n",
    "        if(increase.error < best_network.error and increase.error < decrease.error):\n",
    "            best_network = increase\n",
    "            changes = changes + 1\n",
    "            if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "        elif(decrease.error < best_network.error and decrease.error < increase.error):\n",
    "            best_network = decrease\n",
    "            changes = changes + 1\n",
    "            if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "# W3 is 16x10\n",
    "for i in range(16):\n",
    "    for j in range(10):\n",
    "        weight = best_network.W3[i][j]\n",
    "        best_network.resetError()\n",
    "        increase = best_network.clone()\n",
    "        increase.W3[i][j] = (1+step_size)*weight\n",
    "        decrease = best_network.clone()\n",
    "        decrease.W3[i][j] = (1-step_size)*weight\n",
    "        for image, label in zip(training_images, training_labels):\n",
    "            best_network.identifyNumber(N1=image, trueValue=label)\n",
    "            increase.identifyNumber(N1=image, trueValue=label)\n",
    "            decrease.identifyNumber(N1=image, trueValue=label)\n",
    "        if(increase.error < best_network.error and increase.error < decrease.error):\n",
    "            best_network = increase\n",
    "            changes = changes + 1\n",
    "            if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "        elif(decrease.error < best_network.error and decrease.error < increase.error):\n",
    "            best_network = decrease\n",
    "            changes = changes + 1\n",
    "            if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "# B2 is 16\n",
    "for i in range(16):\n",
    "    bias = best_network.B2[i]\n",
    "    best_network.resetError()\n",
    "    increase = best_network.clone()\n",
    "    increase.B2[i] = (1+step_size)*bias\n",
    "    decrease = best_network.clone()\n",
    "    decrease.B2[i] = (1-step_size)*bias\n",
    "    for image, label in zip(training_images, training_labels):\n",
    "        best_network.identifyNumber(N1=image, trueValue=label)\n",
    "        increase.identifyNumber(N1=image, trueValue=label)\n",
    "        decrease.identifyNumber(N1=image, trueValue=label)\n",
    "    if(increase.error < best_network.error and increase.error < decrease.error):\n",
    "        best_network = increase\n",
    "        changes = changes + 1\n",
    "        if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "    elif(decrease.error < best_network.error and decrease.error < increase.error):\n",
    "        best_network = decrease\n",
    "        changes = changes + 1\n",
    "        if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "# B3 is 16\n",
    "for i in range(16):\n",
    "    bias = best_network.B3[i]\n",
    "    best_network.resetError()\n",
    "    increase = best_network.clone()\n",
    "    increase.B3[i] = (1+step_size)*bias\n",
    "    decrease = best_network.clone()\n",
    "    decrease.B3[i] = (1-step_size)*bias\n",
    "    for image, label in zip(training_images, training_labels):\n",
    "        best_network.identifyNumber(N1=image, trueValue=label)\n",
    "        increase.identifyNumber(N1=image, trueValue=label)\n",
    "        decrease.identifyNumber(N1=image, trueValue=label)\n",
    "    if(increase.error < best_network.error and increase.error < decrease.error):\n",
    "        best_network = increase\n",
    "        changes = changes + 1\n",
    "        if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "    elif(decrease.error < best_network.error and decrease.error < increase.error):\n",
    "        best_network = decrease\n",
    "        changes = changes + 1\n",
    "        if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "# B4 is 10\n",
    "for i in range(10):\n",
    "    bias = best_network.B4[i]\n",
    "    best_network.resetError()\n",
    "    increase = best_network.clone()\n",
    "    increase.B4[i] = (1+step_size)*bias\n",
    "    decrease = best_network.clone()\n",
    "    decrease.B4[i] = (1-step_size)*bias\n",
    "    for image, label in zip(training_images, training_labels):\n",
    "        best_network.identifyNumber(N1=image, trueValue=label)\n",
    "        increase.identifyNumber(N1=image, trueValue=label)\n",
    "        decrease.identifyNumber(N1=image, trueValue=label)\n",
    "    if(increase.error < best_network.error and increase.error < decrease.error):\n",
    "        best_network = increase\n",
    "        changes = changes + 1\n",
    "        if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "    elif(decrease.error < best_network.error and decrease.error < increase.error):\n",
    "        best_network = decrease\n",
    "        changes = changes + 1\n",
    "        if verbose: print(\"Error: {}\".format(best_network.error))\n",
    "print(\"Changes: {}\".format(changes))\n",
    "print(\"Error: {}\".format(best_network.error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After all that work, save the best network so it can be reused in another session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_network.resetError()\n",
    "savefile = os.path.join(\"Data\", \"Best.pkl\")\n",
    "with open(savefile, 'wb') as output:\n",
    "    pickle.dump(best_network, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And run it against the testing set to see if the improvement is meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Mistakes: 1987 out of 11000\n",
      "Error: 0.5011689078124036\n"
     ]
    }
   ],
   "source": [
    "mistakes = 0\n",
    "set_size = testing_images.shape[0]\n",
    "best_network.resetError()\n",
    "for image, label in zip(testing_images, testing_labels):\n",
    "    guess, probs, error = best_network.identifyNumber(N1=image, trueValue=label)\n",
    "    if(label == 99):  # These are the random images, the network should have low confidence all around\n",
    "        if(np.max(probs)>0.5): mistakes = mistakes + 1  # high confidence in any numeral counts as an error\n",
    "    else:  # These are real images, the guess should match the label\n",
    "        if(guess != label): mistakes = mistakes + 1\n",
    "print(\"Total Mistakes: {} out of {}\".format(mistakes, set_size))\n",
    "print(\"Error: {}\".format(best_network.error / set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the champion network on one of my hand-drawn numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, array([  1.77460565e-03,   5.38313184e-06,   1.11935450e-03,\n",
      "         1.17980550e-02,   3.67978335e-02,   4.91742404e-02,\n",
      "         8.40655320e-03,   1.18378643e-02,   9.64306930e-02,\n",
      "         1.42682652e-02]), 443.75654166911278)\n"
     ]
    }
   ],
   "source": [
    "im = cv2.imread(os.path.join(\"Data\",\"One2.png\"), 0)\n",
    "im = np.ndarray.flatten(im)/255.0\n",
    "print(best_network.identifyNumber(N1=im, trueValue=1))\n",
    "best_network.resetError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below are cells used as calculators for my convenience. They can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_i = 0\n",
    "min_val=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12544"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 34392.923737694226, 34392.923737694226, 34392.923737694226\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.5\n",
    "i = 3\n",
    "j = 6\n",
    "weight = best_network.W1[i][j]\n",
    "best_network.resetError()\n",
    "increase = best_network.clone()\n",
    "increase.W1[i][j] = (1+step_size)*weight\n",
    "decrease = best_network.clone()\n",
    "decrease.W1[i][j] = (1-step_size)*weight\n",
    "for image, label in zip(training_images, training_labels):\n",
    "    best_network.identifyNumber(N1=image, trueValue=label)\n",
    "    increase.identifyNumber(N1=image, trueValue=label)\n",
    "    decrease.identifyNumber(N1=image, trueValue=label)\n",
    "print(\"Errors: {}, {}, {}\".format(best_network.error, increase.error, decrease.error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "4.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "increase.W1[i][j] = 4\n",
    "print(increase.W1[i][j])\n",
    "print(decrease.W1[i][j])\n",
    "print(best_network.W1[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
